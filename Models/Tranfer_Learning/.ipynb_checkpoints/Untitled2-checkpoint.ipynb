{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import seaborn as sn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "from graphviz import Source\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_wine\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTE, ADASYN)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "class_names = [\"Normal\", \"Corrupt\", \"Delay\", \"Duplicate\", \"Loss\"]\n",
    "\n",
    "seed = 0\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues, fig_size=(12,10)):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"../../tstat_labels_indexes.txt\" ,'r')\n",
    "data_field_list = []\n",
    "for line in infile.readlines():\n",
    "    if \":\" in line:\n",
    "        data_field = str(re.search('%s(.*)%s' % (\"\\\"\", \"\\\"\"), line).group(1))\n",
    "        index = int(re.search('%s(.*)%s' % (\":\", \",\"), line).group(1))\n",
    "        data_field_list.append((data_field, index))\n",
    "\n",
    "index_to_key_dict = {}\n",
    "key_to_index_dict = {}\n",
    "data_field_labels = []\n",
    "for data_field, index in data_field_list:\n",
    "    key_to_index_dict[data_field] = index\n",
    "    index_to_key_dict[index] = data_field\n",
    "    data_field_labels.append(data_field)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_file(file_name):\n",
    "    infile = open(file_name, 'r')\n",
    "    header = infile.readline().split(' ')\n",
    "    entries = []\n",
    "    labels = None\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        row = get_data_row(line)\n",
    "        row = clean_data_row(row)\n",
    "        if row != []:\n",
    "            entries.append(row)\n",
    "    entries = np.array(entries)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_row(line):\n",
    "    global index_to_key_dict\n",
    "    line = line.split(' ')\n",
    "    row = []\n",
    "    labels = []\n",
    "    c_pkt_cnt = 0\n",
    "    s_pkt_cnt = 0\n",
    "    c_bytes_cnt = 0\n",
    "    s_bytes_cnt = 0\n",
    "    for data_field, index in data_field_list:\n",
    "        #print(\"df:\", data_field,\"ix:\",index)\n",
    "        #print(line)\n",
    "        \n",
    "        \n",
    "        if data_field == \"client_pkt_cnt\":\n",
    "            try:\n",
    "                c_pkt_cnt = line[index]\n",
    "                c_pkt_cnt = max(float(c_pkt_cnt), 1)\n",
    "            except:\n",
    "                c_pkt_cnt = 1\n",
    "            #if c_pkt_cnt < 32:\n",
    "            #    return []\n",
    "        elif data_field == \"serv_pkt_cnt\":\n",
    "            try:\n",
    "                s_pkt_cnt = line[index]\n",
    "                s_pkt_cnt = max(float(s_pkt_cnt), 1)\n",
    "            except:\n",
    "                s_pkt_cnt = 1\n",
    "        elif data_field == \"client_bytes_cnt\":\n",
    "            try:\n",
    "                c_bytes_cnt = line[index]\n",
    "                c_bytes_cnt = max(float(c_bytes_cnt), 1)\n",
    "            except:\n",
    "                c_bytes_cnt = 1\n",
    "        elif data_field == \"serv_bytes_cnt\":\n",
    "            try:\n",
    "                s_bytes_cnt = line[index]\n",
    "                s_bytes_cnt = max(float(s_bytes_cnt), 1)\n",
    "            except:\n",
    "                s_bytes_cnt = 1\n",
    "                \n",
    "    for data_field, index in data_field_list:\n",
    "        try:\n",
    "            val = line[index]\n",
    "            val = float(val)\n",
    "        except:\n",
    "            val = 0\n",
    "        row.append(val)    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_row(in_row):\n",
    "    global index_to_key_dict, key_to_index_dict\n",
    "    return in_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    print(path)\n",
    "    out_data = []\n",
    "    for sub_dir in os.listdir(path):\n",
    "        temp_path = os.path.join(path, sub_dir)\n",
    "        temp_path = os.path.join(temp_path, \"log_tcp_complete\")\n",
    "        if os.path.isfile(temp_path): \n",
    "            temp_data = read_in_file(temp_path)\n",
    "            #print len(temp_data), len(out_data)\n",
    "            if len(temp_data) == 0:\n",
    "                continue\n",
    "            if out_data == []:\n",
    "                out_data = temp_data\n",
    "            else:\n",
    "                out_data = np.concatenate((out_data, temp_data))\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../DataSet/dtn/FINAL_DATA/normal\n",
      "../../DataSet/dtn/DTN_LONG_DATA/normal\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_0.1perc\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_0.5perc\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_1.0perc\n",
      "../../DataSet/dtn/FINAL_DATA/delay_1_var_1\n",
      "../../DataSet/dtn/FINAL_DATA/delay_5_var_2\n",
      "../../DataSet/dtn/FINAL_DATA/delay_10_var_5\n",
      "../../DataSet/dtn/FINAL_DATA/delay_25_var_20\n",
      "../../DataSet/dtn/FINAL_DATA/loss_1perc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pooya/conda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../DataSet/dtn/FINAL_DATA/loss_5perc\n",
      "../../DataSet/dtn/FINAL_DATA/loss_10perc\n",
      "../../DataSet/dtn/FINAL_DATA/loss_15perc\n",
      "../../DataSet/dtn/FINAL_DATA/dup_0.1perc\n",
      "../../DataSet/dtn/FINAL_DATA/dup_1perc\n",
      "../../DataSet/dtn/FINAL_DATA/dup_2perc\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_0.01\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_0.05\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_0.1\n",
      "../../DataSet/dtn/FINAL_DATA/corrupt_0.5\n",
      "../../DataSet/dtn/FINAL_DATA/duplicate_0.01\n",
      "../../DataSet/dtn/FINAL_DATA/duplicate_0.05\n",
      "../../DataSet/dtn/FINAL_DATA/duplicate_0.1\n",
      "../../DataSet/dtn/FINAL_DATA/duplicate_0.5\n",
      "../../DataSet/dtn/FINAL_DATA/loss_0.01\n",
      "../../DataSet/dtn/FINAL_DATA/loss_0.05\n",
      "../../DataSet/dtn/FINAL_DATA/loss_0.1\n",
      "../../DataSet/dtn/FINAL_DATA/loss_0.5\n",
      "(373, 89)\n",
      "(135, 89)\n",
      "(2158, 89)\n",
      "(1079, 89)\n",
      "(47, 89)\n",
      "(24, 89)\n",
      "(47, 89)\n",
      "(24, 89)\n",
      "(47, 89)\n",
      "(24, 89)\n",
      "(39, 89)\n",
      "(17, 89)\n",
      "(42, 89)\n",
      "(19, 89)\n",
      "(42, 89)\n",
      "(19, 89)\n",
      "(35, 89)\n",
      "(14, 89)\n",
      "(238, 89)\n",
      "(119, 89)\n",
      "(299, 89)\n",
      "(119, 89)\n",
      "(191, 89)\n",
      "(74, 89)\n",
      "(183, 89)\n",
      "(68, 89)\n",
      "./hpc/normal\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './hpc/normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7b9720dc3f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./hpc/normal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0mcorr_01\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./hpc/corrupt_0.1perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mcorr_05\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./hpc/corrupt_0.5perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-15382be68ff7>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msub_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_tcp_complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hpc/normal'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dtn_normal = get_dataset(\"../../DataSet/dtn/FINAL_DATA/normal\")\n",
    "dtn_normal2 = get_dataset(\"../../DataSet/dtn/DTN_LONG_DATA/normal\")\n",
    "dtn_corr_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.1perc\")\n",
    "dtn_corr_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.5perc\")\n",
    "dtn_corr_10 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_1.0perc\")\n",
    "dtn_delay_1_1 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_1_var_1\")\n",
    "dtn_delay_5_2 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_5_var_2\")\n",
    "dtn_delay_10_5 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_10_var_5\")\n",
    "dtn_delay_25_20 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_25_var_20\")\n",
    "dtn_drop_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_1perc\")\n",
    "dtn_drop_5 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_5perc\")\n",
    "dtn_drop_10 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_10perc\")\n",
    "dtn_drop_15 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_15perc\")\n",
    "dtn_dup_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_0.1perc\")\n",
    "dtn_dup_1 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_1perc\")\n",
    "dtn_dup_2 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_2perc\")\n",
    "\n",
    "dtn_corrupt_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.01\")\n",
    "dtn_corrupt_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.05\")\n",
    "dtn_corrupt_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.1\")\n",
    "dtn_corrupt_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.5\")\n",
    "dtn_duplicate_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.01\")\n",
    "dtn_duplicate_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.05\")\n",
    "dtn_duplicate_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.1\")\n",
    "dtn_duplicate_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.5\")\n",
    "dtn_loss_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.01\")\n",
    "dtn_loss_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.05\")\n",
    "dtn_loss_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.1\")\n",
    "dtn_loss_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.5\")\n",
    "\n",
    "\n",
    "\n",
    "print(dtn_normal.shape)\n",
    "dtn_normal = dtn_normal[dtn_normal[:,2] > 200]\n",
    "print(dtn_normal.shape)\n",
    "print(dtn_normal2.shape)\n",
    "dtn_normal2 = dtn_normal2[dtn_normal2[:,2] > 200]\n",
    "print(dtn_normal2.shape)\n",
    "print(dtn_corr_01.shape)\n",
    "dtn_corr_01 = dtn_corr_01[dtn_corr_01[:,2] > 200]\n",
    "print(dtn_corr_01.shape)\n",
    "print(dtn_corr_05.shape)\n",
    "dtn_corr_05 = dtn_corr_05[dtn_corr_05[:,2] > 200]\n",
    "print(dtn_corr_05.shape)\n",
    "print(dtn_corr_10.shape)\n",
    "dtn_corr_10 = dtn_corr_10[dtn_corr_10[:,2] > 200]\n",
    "print(dtn_corr_10.shape)\n",
    "\n",
    "print(dtn_delay_1_1.shape)\n",
    "dtn_delay_1_1 = dtn_delay_1_1[dtn_delay_1_1[:,2] > 200]\n",
    "print(dtn_delay_1_1.shape)\n",
    "\n",
    "print(dtn_delay_5_2.shape)\n",
    "dtn_delay_5_2 = dtn_delay_5_2[dtn_delay_5_2[:,2] > 200]\n",
    "print(dtn_delay_5_2.shape)\n",
    "\n",
    "print(dtn_delay_10_5.shape)\n",
    "dtn_delay_10_5 = dtn_delay_10_5[dtn_delay_10_5[:,2] > 200]\n",
    "print(dtn_delay_10_5.shape)\n",
    "\n",
    "print(dtn_delay_25_20.shape)\n",
    "dtn_delay_25_20 = dtn_delay_25_20[dtn_delay_25_20[:,2] > 200]\n",
    "print(dtn_delay_25_20.shape)\n",
    "\n",
    "\n",
    "print(dtn_drop_01.shape)\n",
    "dtn_drop_01 = dtn_drop_01[dtn_drop_01[:,2] > 200]\n",
    "print(dtn_drop_01.shape)\n",
    "\n",
    "print(dtn_drop_5.shape)\n",
    "dtn_drop_5 = dtn_drop_5[dtn_drop_5[:,2] > 200]\n",
    "print(dtn_drop_5.shape)\n",
    "\n",
    "print(dtn_drop_10.shape)\n",
    "dtn_drop_10 = dtn_drop_10[dtn_drop_10[:,2] > 200]\n",
    "print(dtn_drop_10.shape)\n",
    "\n",
    "print(dtn_drop_15.shape)\n",
    "dtn_drop_15 = dtn_drop_15[dtn_drop_15[:,2] > 200]\n",
    "print(dtn_drop_15.shape)\n",
    "\n",
    "dtn_dup_01 = dtn_dup_01[dtn_dup_01[:,2] > 200]\n",
    "dtn_dup_1 = dtn_dup_1[dtn_dup_1[:,2] > 200]\n",
    "dtn_dup_2 = dtn_dup_2[dtn_dup_2[:,2] > 200]\n",
    "\n",
    "\n",
    "\n",
    "dtn_corrupt_001 = dtn_corrupt_001[dtn_corrupt_001[:,2] > 200]\n",
    "dtn_corrupt_005 = dtn_corrupt_005[dtn_corrupt_005[:,2] > 200]\n",
    "dtn_corrupt_01 = dtn_corrupt_01[dtn_corrupt_01[:,2] > 200]\n",
    "dtn_corrupt_05 = dtn_corrupt_05[dtn_corrupt_05[:,2] > 200]\n",
    "dtn_duplicate_001 = dtn_duplicate_001[dtn_duplicate_001[:,2] > 200]\n",
    "dtn_duplicate_005 = dtn_duplicate_005[dtn_duplicate_005[:,2] > 200]\n",
    "dtn_duplicate_01 = dtn_duplicate_01[dtn_duplicate_01[:,2] > 200]\n",
    "dtn_duplicate_05 = dtn_duplicate_05[dtn_duplicate_05[:,2] > 200]\n",
    "dtn_loss_001 = dtn_loss_001[dtn_loss_001[:,2] > 200]\n",
    "dtn_loss_005 = dtn_loss_005[dtn_loss_005[:,2] > 200]\n",
    "dtn_loss_01 = dtn_loss_01[dtn_loss_01[:,2] > 200]\n",
    "dtn_loss_05 = dtn_loss_05[dtn_loss_05[:,2] > 200]\n",
    "\n",
    "\n",
    "#print(dtn_normal)\n",
    "\n",
    "l1  = np.ones(len(dtn_normal  ) + len(dtn_normal2))    *1\n",
    "l2  = np.ones(len(dtn_corr_01  ))   *2\n",
    "l3  = np.ones(len(dtn_corr_05  ))   *2\n",
    "l4  = np.ones(len(dtn_corr_10  ))   *2\n",
    "\n",
    "l5  = np.ones(len(dtn_corrupt_001  ))   *2\n",
    "l6  = np.ones(len(dtn_corrupt_005  ))   *2\n",
    "l7  = np.ones(len(dtn_corrupt_01  ))   *2\n",
    "l8  = np.ones(len(dtn_corrupt_05  ))   *2\n",
    "\n",
    "l9  = np.ones(len(dtn_delay_1_1))   *3\n",
    "l10  = np.ones(len(dtn_delay_5_2))   *3\n",
    "l11  = np.ones(len(dtn_delay_10_5))  *3\n",
    "l12  = np.ones(len(dtn_delay_25_20)) *3\n",
    "l13  = np.ones(len(dtn_drop_01 ) )     *4\n",
    "l14  = np.ones(len(dtn_drop_5) )    *4\n",
    "l15  = np.ones(len(dtn_drop_10) )   *4\n",
    "l16 = np.ones(len(dtn_drop_15) )   *4\n",
    "\n",
    "l17  = np.ones(len(dtn_loss_001))       *4\n",
    "l18  = np.ones(len(dtn_loss_005))       *4\n",
    "l19  = np.ones(len(dtn_loss_01))       *4\n",
    "l20  = np.ones(len(dtn_loss_05))       *4\n",
    "\n",
    "l21  = np.ones(len(dtn_dup_01))      *5\n",
    "l22  = np.ones(len(dtn_dup_1))       *5\n",
    "l23  = np.ones(len(dtn_dup_2))       *5\n",
    "\n",
    "l24  = np.ones(len(dtn_duplicate_001))      *5\n",
    "l25  = np.ones(len(dtn_duplicate_005))      *5\n",
    "l26  = np.ones(len(dtn_duplicate_01))      *5\n",
    "l27  = np.ones(len(dtn_duplicate_05))      *5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtn_data = np.concatenate((dtn_normal, dtn_normal2,\n",
    "                           dtn_corr_01, dtn_corr_05, dtn_corr_10\n",
    "                           ,dtn_corrupt_001,\n",
    "                            dtn_corrupt_005,\n",
    "                            dtn_corrupt_01 ,\n",
    "                            dtn_corrupt_05 \n",
    "                           ,dtn_delay_1_1, dtn_delay_5_2,dtn_delay_10_5,dtn_delay_25_20,\n",
    "                           dtn_drop_01, dtn_drop_5, dtn_drop_10, dtn_drop_15, dtn_loss_001,\n",
    "dtn_loss_005, \n",
    "dtn_loss_01, \n",
    "dtn_loss_05,                          \n",
    "                           dtn_dup_01,dtn_dup_1, dtn_dup_2,dtn_duplicate_001,\n",
    "dtn_duplicate_005,\n",
    "dtn_duplicate_01,\n",
    "dtn_duplicate_05,))\n",
    "\n",
    "\n",
    "pandas_dtn = pd.DataFrame(data=dtn_data, \n",
    "              columns=data_field_labels)\n",
    "dtn_data = MinMaxScaler().fit_transform(pandas_dtn)\n",
    "\n",
    "\n",
    "dtn_anom_type_data_labels = np.concatenate((l1, l2, l3, l4, l5, l6, l7, l8, l9, l10, l11, l12, l13, l14,l15,\n",
    "                                           l16, l17, l18, l19, l20, l21,l22, l23, l24,l25,l26,l27))\n",
    "dtn_anom_type_data_labels = pd.DataFrame(data=dtn_anom_type_data_labels)\n",
    "dtn_anom_type_data_labels = dtn_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "normal = get_dataset(\"./hpc/normal\")\n",
    "corr_01 = get_dataset(\"./hpc/corrupt_0.1perc\")\n",
    "corr_05 = get_dataset(\"./hpc/corrupt_0.5perc\")\n",
    "corr_10 = get_dataset(\"./hpc/corrupt_1.0perc\")\n",
    "delay_1_1 = get_dataset(\"./hpc/delay_1_var_1\")\n",
    "delay_5_2 = get_dataset(\"./hpc/delay_5_var_2\")\n",
    "delay_10_5 = get_dataset(\"./hpc/delay_10_var_5\")\n",
    "delay_25_20 = get_dataset(\"./hpc/delay_25_var_20\")\n",
    "drop_01 = get_dataset(\"./hpc/loss_5perc\")\n",
    "drop_001 = get_dataset(\"./hpc/loss_10perc\")\n",
    "drop_0005 = get_dataset(\"./hpc/loss_15perc\")\n",
    "dup_1 = get_dataset(\"./hpc/dup_10perc\")\n",
    "dup_2 = get_dataset(\"./hpc/dup_20perc\")\n",
    "\n",
    "\n",
    "print(normal.shape)\n",
    "normal = normal[normal[:,2] > 200]\n",
    "print(normal.shape)\n",
    "\n",
    "print(corr_01.shape)\n",
    "corr_01 = corr_01[corr_01[:,2] > 200]\n",
    "print(corr_01.shape)\n",
    "\n",
    "print(corr_05.shape)\n",
    "corr_05 = corr_05[corr_05[:,2] > 200]\n",
    "print(corr_05.shape)\n",
    "\n",
    "print(corr_10.shape)\n",
    "corr_10 = corr_10[corr_10[:,2] > 200]\n",
    "print(corr_10.shape)\n",
    "\n",
    "print(delay_1_1.shape)\n",
    "delay_1_1 = delay_1_1[delay_1_1[:,2] > 200]\n",
    "print(delay_1_1.shape)\n",
    "\n",
    "print(delay_5_2.shape)\n",
    "delay_5_2 = delay_5_2[delay_5_2[:,2] > 200]\n",
    "print(delay_5_2.shape)\n",
    "\n",
    "print(delay_10_5.shape)\n",
    "delay_10_5 = delay_10_5[delay_10_5[:,2] > 200]\n",
    "print(delay_10_5.shape)\n",
    "\n",
    "print(delay_25_20.shape)\n",
    "delay_25_20 = delay_25_20[delay_25_20[:,2] > 200]\n",
    "print(delay_25_20.shape)\n",
    "\n",
    "print(drop_01.shape)\n",
    "drop_01 = drop_01[drop_01[:,2] > 200]\n",
    "print(drop_01.shape)\n",
    "\n",
    "print(drop_001.shape)\n",
    "drop_001 = drop_001[drop_001[:,2] > 200]\n",
    "print(drop_001.shape)\n",
    "\n",
    "print(drop_0005.shape)\n",
    "drop_0005 = drop_0005[drop_0005[:,2] > 200]\n",
    "print(drop_0005.shape)\n",
    "\n",
    "print(dup_1.shape)\n",
    "dup_1 = dup_1[dup_1[:,2] > 200]\n",
    "print(dup_1.shape)\n",
    "\n",
    "print(dup_2.shape)\n",
    "dup_2 = dup_2[dup_2[:,2] > 200]\n",
    "print(dup_2.shape)\n",
    "\n",
    "hpc_normal = np.nan_to_num(normal)\n",
    "hpc_corr_01 = np.nan_to_num(corr_01)\n",
    "hpc_corr_05 =  np.nan_to_num(corr_05)\n",
    "hpc_corr_10 = np.nan_to_num(corr_10)\n",
    "hpc_delay_1_1 = np.nan_to_num(delay_1_1)\n",
    "hpc_delay_5_2 = np.nan_to_num(delay_5_2)\n",
    "hpc_delay_10_5 = np.nan_to_num(delay_10_5)\n",
    "hpc_delay_25_20 = np.nan_to_num(delay_25_20)\n",
    "hpc_drop_01 = np.nan_to_num(drop_01)\n",
    "hpc_drop_001 = np.nan_to_num(drop_001)\n",
    "hpc_drop_0005 = np.nan_to_num(drop_0005)\n",
    "hpc_dup_1 = np.nan_to_num(dup_1)\n",
    "hpc_dup_2 = np.nan_to_num(dup_2)\n",
    "\n",
    "\n",
    "hpc_data = np.concatenate((hpc_normal, \n",
    "                           hpc_corr_01, hpc_corr_05, hpc_corr_10,\n",
    "                           hpc_delay_1_1, hpc_delay_5_2,hpc_delay_10_5,hpc_delay_25_20,\n",
    "                           hpc_drop_01, hpc_drop_001, hpc_drop_0005,\n",
    "                           hpc_dup_1, hpc_dup_2))\n",
    "\n",
    "\n",
    "a_labels  = np.ones(len(hpc_normal  ))    *1\n",
    "b_labels  = np.ones(len(hpc_corr_01  ))   *2\n",
    "c_labels  = np.ones(len(hpc_corr_05  ))   *2\n",
    "d_labels  = np.ones(len(hpc_corr_10  ))   *2\n",
    "e_labels  = np.ones(len(hpc_delay_1_1))   *3\n",
    "f_labels  = np.ones(len(hpc_delay_5_2))   *3\n",
    "g_labels  = np.ones(len(hpc_delay_10_5))  *3\n",
    "h_labels  = np.ones(len(hpc_delay_25_20)) *3\n",
    "i_labels  = np.ones(len(hpc_drop_01))     *4\n",
    "j_labels  = np.ones(len(hpc_drop_001))    *4\n",
    "k_labels  = np.ones(len(hpc_drop_0005))   *4\n",
    "m_labels  = np.ones(len(hpc_dup_1))       *5\n",
    "n_labels  = np.ones(len(hpc_dup_2))       *5\n",
    "\n",
    "hpc_anom_type_data_labels = np.concatenate((a_labels, b_labels, c_labels, d_labels, e_labels, \n",
    "                                            f_labels, g_labels, h_labels, \n",
    "                                            i_labels, j_labels, k_labels, \n",
    "                                            m_labels, n_labels))\n",
    "\n",
    "\n",
    "pandas_hpc = pd.DataFrame(data=hpc_data, \n",
    "              columns=data_field_labels)\n",
    "hpc_data = MinMaxScaler().fit_transform(pandas_hpc)\n",
    "\n",
    "\n",
    "hpc_anom_type_data_labels = pd.DataFrame(data=hpc_anom_type_data_labels)\n",
    "hpc_anom_type_data_labels = hpc_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "print(pandas_hpc.shape)\n",
    "print(hpc_anom_type_data_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Datasets (randomized on seed value)¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hpc_train_data, hpc_test_data, hpc_train_labels, hpc_test_labels = train_test_split(hpc_data, \n",
    "                                                                    hpc_anom_type_data_labels, random_state=seed)\n",
    "hpc_train_data, hpc_train_labels = SMOTE().fit_resample(hpc_train_data, hpc_train_labels)\n",
    "\n",
    "dtn_train_data, dtn_test_data, dtn_train_labels, dtn_test_labels = train_test_split(dtn_data, \n",
    "                                                                    dtn_anom_type_data_labels, random_state=seed)\n",
    "dtn_train_data, dtn_train_labels = SMOTE().fit_resample(dtn_train_data, dtn_train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Networks :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpc =  DecisionTreeClassifier()\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n",
    "hpc_predicted_labels = hpc.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "dtn_predicted_labels = hpc.predict(dtn_test_data)\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RF :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hpc =  RandomForestClassifier(n_estimators = 1000, random_state = 42,n_jobs = -1)\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n",
    "hpc_predicted_labels = hpc.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_RF.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "hpc = svm.SVC(kernel='linear', gamma ='auto', max_iter=10000)\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n",
    "hpc_predicted_labels = hpc.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_SVM_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_SVM.pdf')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## NN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = hpc_train_data.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(700, activation='relu', input_dim=n_cols))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "hpc_train_labels = to_categorical(hpc_train_labels)\n",
    "#dtn_train_labels = to_categorical(dtn_train_labels)\n",
    "\n",
    "model.fit(hpc_train_data, hpc_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels,  np.argmax(hpc_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "plot_confusion_matrix(dtn_test_labels,  np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.show()\n",
    "\n",
    "model.fit(dtn_train_data, dtn_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "plot_confusion_matrix(hpc_test_labels,  np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
