{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Indexes\n",
    "This chunk loads a file that contains the labels we want to load from the datasets as well as their indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import seaborn as sn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "from graphviz import Source\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_wine\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTE, ADASYN)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "class_names = [\"Normal\", \"Corrupt\", \"Delay\", \"Duplicate\", \"Loss\"]\n",
    "\n",
    "seed = 0\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues, fig_size=(12,10)):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"../../tstat_labels_indexes.txt\" ,'r')\n",
    "data_field_list = []\n",
    "for line in infile.readlines():\n",
    "    if \":\" in line:\n",
    "        data_field = str(re.search('%s(.*)%s' % (\"\\\"\", \"\\\"\"), line).group(1))\n",
    "        index = int(re.search('%s(.*)%s' % (\":\", \",\"), line).group(1))\n",
    "        data_field_list.append((data_field, index))\n",
    "\n",
    "index_to_key_dict = {}\n",
    "key_to_index_dict = {}\n",
    "data_field_labels = []\n",
    "for data_field, index in data_field_list:\n",
    "    key_to_index_dict[data_field] = index\n",
    "    index_to_key_dict[index] = data_field\n",
    "    data_field_labels.append(data_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_file(file_name):\n",
    "    infile = open(file_name, 'r')\n",
    "    header = infile.readline().split(' ')\n",
    "    entries = []\n",
    "    labels = None\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        row = get_data_row(line)\n",
    "        row = clean_data_row(row)\n",
    "        if row != []:\n",
    "            entries.append(row)\n",
    "    entries = np.array(entries)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data row\n",
    "Called by the read in file function. Loads a single line from the dataset files. Super inefficient, but only loads labels which are in the data field list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_row(line):\n",
    "    global index_to_key_dict\n",
    "    line = line.split(' ')\n",
    "    row = []\n",
    "    labels = []\n",
    "    c_pkt_cnt = 0\n",
    "    s_pkt_cnt = 0\n",
    "    c_bytes_cnt = 0\n",
    "    s_bytes_cnt = 0\n",
    "    for data_field, index in data_field_list:\n",
    "        #print(\"df:\", data_field,\"ix:\",index)\n",
    "        #print(line)\n",
    "        \n",
    "        \n",
    "        if data_field == \"client_pkt_cnt\":\n",
    "            try:\n",
    "                c_pkt_cnt = line[index]\n",
    "                c_pkt_cnt = max(float(c_pkt_cnt), 1)\n",
    "            except:\n",
    "                c_pkt_cnt = 1\n",
    "            #if c_pkt_cnt < 32:\n",
    "            #    return []\n",
    "        elif data_field == \"serv_pkt_cnt\":\n",
    "            try:\n",
    "                s_pkt_cnt = line[index]\n",
    "                s_pkt_cnt = max(float(s_pkt_cnt), 1)\n",
    "            except:\n",
    "                s_pkt_cnt = 1\n",
    "        elif data_field == \"client_bytes_cnt\":\n",
    "            try:\n",
    "                c_bytes_cnt = line[index]\n",
    "                c_bytes_cnt = max(float(c_bytes_cnt), 1)\n",
    "            except:\n",
    "                c_bytes_cnt = 1\n",
    "        elif data_field == \"serv_bytes_cnt\":\n",
    "            try:\n",
    "                s_bytes_cnt = line[index]\n",
    "                s_bytes_cnt = max(float(s_bytes_cnt), 1)\n",
    "            except:\n",
    "                s_bytes_cnt = 1\n",
    "                \n",
    "    for data_field, index in data_field_list:\n",
    "        try:\n",
    "            val = line[index]\n",
    "            val = float(val)\n",
    "        except:\n",
    "            val = 0\n",
    "        row.append(val)    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data row\n",
    "Not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_row(in_row):\n",
    "    global index_to_key_dict, key_to_index_dict\n",
    "    return in_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset\n",
    "Loads all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_count_threshold = 200\n",
    "def get_dataset(path):\n",
    "    out_data = []\n",
    "    file = open('summary.out', 'w')\n",
    "    for sub_dir in os.listdir(path):\n",
    "        temp_path = os.path.join(path, sub_dir)\n",
    "        temp_path = os.path.join(temp_path, \"log_tcp_complete\")\n",
    "        if os.path.isfile(temp_path): \n",
    "            temp_data = np.nan_to_num(read_in_file(temp_path))\n",
    "            rows_before = temp_data.shape[0]\n",
    "            if rows_before == 0:\n",
    "                continue\n",
    "            print()\n",
    "            temp_data = temp_data[temp_data[:,2] > packet_count_threshold]\n",
    "            rows_after = temp_data.shape[0]\n",
    "            print (path + \" removed \" + str(rows_before-rows_after) + \"/\" + str(rows_before) + \" rows\")\n",
    "            if len(temp_data) == 0:\n",
    "                continue\n",
    "            if out_data == []:\n",
    "                out_data = temp_data\n",
    "            else:\n",
    "                out_data = np.concatenate((out_data, temp_data))\n",
    "    file.close\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all datasets\n",
    "Load all datasets\n",
    "Create numerical lables for each class, and a different set of labels for each subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/normal removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_0.1perc removed 119/190 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_0.1perc removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_0.1perc removed 118/236 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_0.1perc removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_0.1perc removed 119/238 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_5perc removed 121/235 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_5perc removed 120/233 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_5perc removed 94/147 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pooya/conda/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../../DataSet/emulab/corrupt_5perc removed 99/165 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_5perc removed 121/202 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_1.0perc removed 104/207 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_1.0perc removed 119/239 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_1.0perc removed 119/236 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_1.0perc removed 119/193 rows\n",
      "\n",
      "../../DataSet/emulab/corrupt_1.0perc removed 119/235 rows\n",
      "\n",
      "../../DataSet/emulab/delay_1_var_1 removed 117/192 rows\n",
      "\n",
      "../../DataSet/emulab/delay_1_var_1 removed 118/189 rows\n",
      "\n",
      "../../DataSet/emulab/delay_1_var_1 removed 118/195 rows\n",
      "\n",
      "../../DataSet/emulab/delay_1_var_1 removed 118/188 rows\n",
      "\n",
      "../../DataSet/emulab/delay_1_var_1 removed 118/190 rows\n",
      "\n",
      "../../DataSet/emulab/delay_5_var_2 removed 117/224 rows\n",
      "\n",
      "../../DataSet/emulab/delay_5_var_2 removed 119/210 rows\n",
      "\n",
      "../../DataSet/emulab/delay_5_var_2 removed 118/207 rows\n",
      "\n",
      "../../DataSet/emulab/delay_5_var_2 removed 115/199 rows\n",
      "\n",
      "../../DataSet/emulab/delay_5_var_2 removed 118/210 rows\n",
      "\n",
      "../../DataSet/emulab/delay_10_var_5 removed 118/201 rows\n",
      "\n",
      "../../DataSet/emulab/delay_10_var_5 removed 119/192 rows\n",
      "\n",
      "../../DataSet/emulab/delay_10_var_5 removed 119/191 rows\n",
      "\n",
      "../../DataSet/emulab/delay_10_var_5 removed 119/195 rows\n",
      "\n",
      "../../DataSet/emulab/delay_10_var_5 removed 117/189 rows\n",
      "\n",
      "../../DataSet/emulab/delay_25_var_20 removed 119/187 rows\n",
      "\n",
      "../../DataSet/emulab/delay_25_var_20 removed 119/193 rows\n",
      "\n",
      "../../DataSet/emulab/delay_25_var_20 removed 119/195 rows\n",
      "\n",
      "../../DataSet/emulab/delay_25_var_20 removed 119/191 rows\n",
      "\n",
      "../../DataSet/emulab/delay_25_var_20 removed 119/189 rows\n",
      "\n",
      "../../DataSet/emulab/loss_1_perc removed 119/230 rows\n",
      "\n",
      "../../DataSet/emulab/loss_1_perc removed 117/234 rows\n",
      "\n",
      "../../DataSet/emulab/loss_1_perc removed 119/199 rows\n",
      "\n",
      "../../DataSet/emulab/loss_1_perc removed 119/236 rows\n",
      "\n",
      "../../DataSet/emulab/loss_1_perc removed 119/236 rows\n",
      "\n",
      "../../DataSet/emulab/loss_5_perc removed 119/190 rows\n",
      "\n",
      "../../DataSet/emulab/loss_5_perc removed 102/157 rows\n",
      "\n",
      "../../DataSet/emulab/loss_5_perc removed 119/226 rows\n",
      "\n",
      "../../DataSet/emulab/loss_5_perc removed 119/235 rows\n",
      "\n",
      "../../DataSet/emulab/loss_5_perc removed 119/232 rows\n",
      "\n",
      "../../DataSet/emulab/loss_10_perc removed 119/201 rows\n",
      "\n",
      "../../DataSet/emulab/loss_10_perc removed 100/152 rows\n",
      "\n",
      "../../DataSet/emulab/loss_10_perc removed 118/222 rows\n",
      "\n",
      "../../DataSet/emulab/loss_10_perc removed 117/221 rows\n",
      "\n",
      "../../DataSet/emulab/loss_10_perc removed 110/190 rows\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../DataSet/emulab/loss_15perc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-943e11e3ddeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdrop_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../DataSet/emulab/loss_5_perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdrop_6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../DataSet/emulab/loss_10_perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdrop_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../DataSet/emulab/loss_15perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdup_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../DataSet/emulab/dup_1perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdup_5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../DataSet/emulab/dup_5perc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-01c311f8c953>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mout_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summary.out'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msub_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log_tcp_complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../DataSet/emulab/loss_15perc'"
     ]
    }
   ],
   "source": [
    "normal = get_dataset(\"../../DataSet/emulab/normal\")\n",
    "corr_01 = get_dataset(\"../../DataSet/emulab/corrupt_0.1perc\")\n",
    "corr_05 =  get_dataset(\"../../DataSet/emulab/corrupt_5perc\")\n",
    "corr_10 = get_dataset(\"../../DataSet/emulab/corrupt_1.0perc\")\n",
    "delay_1_1 = get_dataset(\"../../DataSet/emulab/delay_1_var_1\")\n",
    "delay_5_2 = get_dataset(\"../../DataSet/emulab/delay_5_var_2\")\n",
    "delay_10_5 = get_dataset(\"../../DataSet/emulab/delay_10_var_5\")\n",
    "delay_25_20 = get_dataset(\"../../DataSet/emulab/delay_25_var_20\")\n",
    "drop_1 = get_dataset(\"../../DataSet/emulab/loss_1_perc\")\n",
    "drop_3 = get_dataset(\"../../DataSet/emulab/loss_5_perc\")\n",
    "drop_6 = get_dataset(\"../../DataSet/emulab/loss_10_perc\")\n",
    "dup_1 = get_dataset(\"../../DataSet/emulab/dup_1perc\")\n",
    "dup_5 = get_dataset(\"../../DataSet/emulab/dup_5perc\")\n",
    "dup_7 = get_dataset(\"../../DataSet/emulab/dup_7perc\")\n",
    "\n",
    "\n",
    "\n",
    "l1  = np.ones(len(normal  ))    *1\n",
    "l2  = np.ones(len(corr_01  ))   *2\n",
    "l3  = np.ones(len(corr_05  ))   *2\n",
    "l4  = np.ones(len(corr_10  ))   *2\n",
    "l5  = np.ones(len(delay_1_1))   *3\n",
    "l6  = np.ones(len(delay_5_2))   *3\n",
    "l7  = np.ones(len(delay_10_5))  *3\n",
    "l8  = np.ones(len(delay_25_20)) *3\n",
    "l9  = np.ones(len(drop_1 ) )    *4\n",
    "l10  = np.ones(len(drop_3 ) )   *4\n",
    "l11  = np.ones(len(drop_6 ) )   *4\n",
    "l12  = np.ones(len(dup_1))      *5\n",
    "l13  = np.ones(len(dup_5))      *5\n",
    "l14  = np.ones(len(dup_7))      *5\n",
    "\n",
    "\n",
    "\n",
    "emulab_data = np.concatenate((normal, corr_01, corr_05, corr_10, delay_1_1, delay_5_2, delay_10_5, delay_25_20,\n",
    "                           drop_1, drop_3, drop_6, dup_1, dup_5, dup_7))\n",
    "print(\"###########\")\n",
    "print(emulab_data.shape)\n",
    "print(\"###########\")\n",
    "pandas_emulab = pd.DataFrame(data=emulab_data, \n",
    "              columns=data_field_labels)\n",
    "emulab_data = MinMaxScaler().fit_transform(pandas_emulab)\n",
    "\n",
    "\n",
    "emulab_anom_type_data_labels = np.concatenate((l1, l2, l3, l4, l5, l6, l7, l8, l9, l10, l11, l12, l13, l14))\n",
    "emulab_anom_type_data_labels = pd.DataFrame(data=emulab_anom_type_data_labels)\n",
    "emulab_anom_type_data_labels = emulab_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hpc_normal = get_dataset(\"../../DataSet/hpc/normal\")\n",
    "hpc_corr_01 = get_dataset(\"../../DataSet/hpc/corrupt_0.1perc\")\n",
    "hpc_corr_05 = get_dataset(\"../../DataSet/hpc/corrupt_0.5perc\")\n",
    "hpc_corr_10 = get_dataset(\"../../DataSet/hpc/corrupt_1.0perc\")\n",
    "hpc_delay_1_1 = get_dataset(\"../../DataSet/hpc/delay_1_var_1\")\n",
    "hpc_delay_5_2 = get_dataset(\"../../DataSet/hpc/delay_5_var_2\")\n",
    "hpc_delay_10_5 = get_dataset(\"../../DataSet/hpc/delay_10_var_5\")\n",
    "hpc_delay_25_20 = get_dataset(\"../../DataSet/hpc/delay_25_var_20\")\n",
    "hpc_drop_01 = get_dataset(\"../../DataSet/hpc/loss_5perc\")\n",
    "hpc_drop_001 = get_dataset(\"../../DataSet/hpc/loss_10perc\")\n",
    "hpc_drop_0005 = get_dataset(\"../../DataSet/hpc/loss_15perc\")\n",
    "hpc_dup_1 = get_dataset(\"../../DataSet/hpc/dup_10perc\")\n",
    "hpc_dup_2 = get_dataset(\"../../DataSet/hpc/dup_20perc\")\n",
    "\n",
    "\n",
    "hpc_corrupt_001 = get_dataset(\"../../DataSet/hpc/corrupt_0.01\")\n",
    "hpc_corrupt_005 = get_dataset(\"../../DataSet/hpc/corrupt_0.05\")\n",
    "hpc_corrupt_01 = get_dataset(\"../../DataSet/hpc/corrupt_0.1\")\n",
    "hpc_corrupt_05 = get_dataset(\"../../DataSet/hpc/corrupt_0.5\")\n",
    "hpc_duplicate_001 = get_dataset(\"../../DataSet/hpc/duplicate_0.01\")\n",
    "hpc_duplicate_005 = get_dataset(\"../../DataSet/hpc/duplicate_0.05\")\n",
    "hpc_duplicate_01 = get_dataset(\"../../DataSet/hpc/duplicate_0.1\")\n",
    "hpc_duplicate_05 = get_dataset(\"../../DataSet/hpc/duplicate_0.5\")\n",
    "hpc_loss_001 = get_dataset(\"../../DataSet/hpc/loss_0.01\")\n",
    "hpc_loss_005 = get_dataset(\"../../DataSet/hpc/loss_0.05\")\n",
    "hpc_loss_01 = get_dataset(\"../../DataSet/hpc/loss_0.1\")\n",
    "hpc_loss_05 = get_dataset(\"../../DataSet/hpc/loss_0.5\")\n",
    "\n",
    "\n",
    "\n",
    "hpc_l1  = np.ones(len(hpc_normal  ))    *1\n",
    "hpc_l2  = np.ones(len(hpc_corr_01  ))   *2\n",
    "hpc_l3  = np.ones(len(hpc_corr_05  ))   *2\n",
    "hpc_l4  = np.ones(len(hpc_corr_10  ))   *2\n",
    "hpc_l5  = np.ones(len(hpc_corrupt_001  ))   *2\n",
    "hpc_l6  = np.ones(len(hpc_corrupt_005  ))   *2\n",
    "hpc_l7  = np.ones(len(hpc_corrupt_01  ))   *2\n",
    "hpc_l8  = np.ones(len(hpc_corrupt_05  ))   *2\n",
    "hpc_l9  = np.ones(len(hpc_delay_1_1))   *3\n",
    "hpc_l10  = np.ones(len(hpc_delay_5_2))   *3\n",
    "hpc_l11  = np.ones(len(hpc_delay_10_5))  *3\n",
    "hpc_l12  = np.ones(len(hpc_delay_25_20)) *3\n",
    "hpc_l13  = np.ones(len(hpc_drop_01))     *4\n",
    "hpc_l14  = np.ones(len(hpc_drop_001))    *4\n",
    "hpc_l15  = np.ones(len(hpc_drop_0005))   *4\n",
    "hpc_l16  = np.ones(len(hpc_loss_001))   *4\n",
    "hpc_l17  = np.ones(len(hpc_loss_005))   *4\n",
    "hpc_l18  = np.ones(len(hpc_loss_01))   *4\n",
    "hpc_l19  = np.ones(len(hpc_loss_05))   *4\n",
    "hpc_l20  = np.ones(len(hpc_dup_1))       *5\n",
    "hpc_l21  = np.ones(len(hpc_dup_2))       *5\n",
    "hpc_l22  = np.ones(len(hpc_duplicate_001))       *5\n",
    "hpc_l23  = np.ones(len(hpc_duplicate_005))       *5\n",
    "hpc_l24  = np.ones(len(hpc_duplicate_01))       *5\n",
    "hpc_l25  = np.ones(len(hpc_duplicate_05))       *5\n",
    "\n",
    "hpc_data = np.concatenate((hpc_normal, \n",
    "                           hpc_corr_01, hpc_corr_05, hpc_corr_10,\n",
    "                           hpc_corrupt_001,hpc_corrupt_005,hpc_corrupt_01,hpc_corrupt_05,\n",
    "                           hpc_delay_1_1, hpc_delay_5_2,hpc_delay_10_5,hpc_delay_25_20,\n",
    "                           hpc_drop_01, hpc_drop_001, hpc_drop_0005,\n",
    "                           hpc_loss_001,hpc_loss_005,hpc_loss_01,hpc_loss_05,\n",
    "                           hpc_dup_1, hpc_dup_2,hpc_duplicate_001,hpc_duplicate_005,hpc_duplicate_01,hpc_duplicate_05))\n",
    "\n",
    "hpc_anom_type_data_labels = np.concatenate((hpc_l1, hpc_l2, hpc_l3, hpc_l4, hpc_l5, \n",
    "                                            hpc_l6, hpc_l7, hpc_l8, \n",
    "                                            hpc_l9, hpc_l10, hpc_l11, \n",
    "                                            hpc_l12, hpc_l13,hpc_l14, hpc_l15, hpc_l16, \n",
    "                                            hpc_l17, hpc_l18,hpc_l19,hpc_l20, \n",
    "                                            hpc_l21, hpc_l22,hpc_l23,hpc_l24,hpc_l25))\n",
    "\n",
    "pandas_hpc = pd.DataFrame(data=hpc_data, \n",
    "              columns=data_field_labels)\n",
    "hpc_data = MinMaxScaler().fit_transform(pandas_hpc)\n",
    "\n",
    "\n",
    "hpc_anom_type_data_labels = pd.DataFrame(data=hpc_anom_type_data_labels)\n",
    "hpc_anom_type_data_labels = hpc_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "print(pandas_hpc.shape)\n",
    "print(hpc_anom_type_data_labels.shape)\n",
    "\n",
    "dtn_normal = get_dataset(\"../../DataSet/dtn/FINAL_DATA/normal\")\n",
    "dtn_normal2 = get_dataset(\"../../DataSet/dtn/DTN_LONG_DATA/normal\")\n",
    "dtn_corr_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.1perc\")\n",
    "dtn_corr_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.5perc\")\n",
    "dtn_corr_10 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_1.0perc\")\n",
    "dtn_delay_1_1 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_1_var_1\")\n",
    "dtn_delay_5_2 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_5_var_2\")\n",
    "dtn_delay_10_5 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_10_var_5\")\n",
    "dtn_delay_25_20 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_25_var_20\")\n",
    "dtn_drop_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_1perc\")\n",
    "dtn_drop_5 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_5perc\")\n",
    "dtn_drop_10 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_10perc\")\n",
    "dtn_drop_15 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_15perc\")\n",
    "dtn_dup_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_0.1perc\")\n",
    "dtn_dup_1 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_1perc\")\n",
    "dtn_dup_2 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_2perc\")\n",
    "\n",
    "dtn_corrupt_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.01\")\n",
    "dtn_corrupt_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.05\")\n",
    "dtn_corrupt_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.1\")\n",
    "dtn_corrupt_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.5\")\n",
    "dtn_duplicate_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.01\")\n",
    "dtn_duplicate_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.05\")\n",
    "dtn_duplicate_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.1\")\n",
    "dtn_duplicate_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.5\")\n",
    "dtn_loss_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.01\")\n",
    "dtn_loss_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.05\")\n",
    "dtn_loss_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.1\")\n",
    "dtn_loss_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.5\")\n",
    "\n",
    "\n",
    "l1  = np.ones(len(dtn_normal  ) + len(dtn_normal2)) *1\n",
    "l2  = np.ones(len(dtn_corr_01  ))   *2\n",
    "l3  = np.ones(len(dtn_corr_05  ))   *2\n",
    "l4  = np.ones(len(dtn_corr_10  ))   *2\n",
    "l5  = np.ones(len(dtn_corrupt_001  ))  *2\n",
    "l6  = np.ones(len(dtn_corrupt_005  ))  *2\n",
    "l7  = np.ones(len(dtn_corrupt_01  ))   *2\n",
    "l8  = np.ones(len(dtn_corrupt_05  ))   *2\n",
    "l9  = np.ones(len(dtn_delay_1_1))   *3\n",
    "l10  = np.ones(len(dtn_delay_5_2))   *3\n",
    "l11  = np.ones(len(dtn_delay_10_5))  *3\n",
    "l12  = np.ones(len(dtn_delay_25_20)) *3\n",
    "l13  = np.ones(len(dtn_drop_01 ) ) *4\n",
    "l14  = np.ones(len(dtn_drop_5) )   *4\n",
    "l15  = np.ones(len(dtn_drop_10) )  *4\n",
    "l16 = np.ones(len(dtn_drop_15) )   *4\n",
    "l17  = np.ones(len(dtn_loss_001))  *4\n",
    "l18  = np.ones(len(dtn_loss_005))  *4\n",
    "l19  = np.ones(len(dtn_loss_01))   *4\n",
    "l20  = np.ones(len(dtn_loss_05))   *4\n",
    "l21  = np.ones(len(dtn_dup_01))    *5\n",
    "l22  = np.ones(len(dtn_dup_1))     *5\n",
    "l23  = np.ones(len(dtn_dup_2))     *5\n",
    "l24  = np.ones(len(dtn_duplicate_001)) *5\n",
    "l25  = np.ones(len(dtn_duplicate_005))*5\n",
    "l26  = np.ones(len(dtn_duplicate_01)) *5\n",
    "l27  = np.ones(len(dtn_duplicate_05)) *5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtn_data = np.concatenate((dtn_normal, dtn_normal2,\n",
    "                           dtn_corr_01, dtn_corr_05, dtn_corr_10,\n",
    "                           dtn_corrupt_001, dtn_corrupt_005, dtn_corrupt_01, dtn_corrupt_05, \n",
    "                           dtn_delay_1_1, dtn_delay_5_2,dtn_delay_10_5,dtn_delay_25_20,\n",
    "                           dtn_drop_01, dtn_drop_5, dtn_drop_10, dtn_drop_15, dtn_loss_001,\n",
    "                           dtn_loss_005, dtn_loss_01, dtn_loss_05,                          \n",
    "                           dtn_dup_01,dtn_dup_1, dtn_dup_2,dtn_duplicate_001,\n",
    "                           dtn_duplicate_005, dtn_duplicate_01, dtn_duplicate_05))\n",
    "print(\"###########\")\n",
    "print(data_field_labels)\n",
    "print(\"###########\")\n",
    "pandas_dtn = pd.DataFrame(data=dtn_data, \n",
    "              columns=data_field_labels)\n",
    "dtn_data = MinMaxScaler().fit_transform(pandas_dtn)\n",
    "\n",
    "\n",
    "dtn_anom_type_data_labels = np.concatenate((l1, l2, l3, l4, l5, l6, l7, l8, l9, l10, l11, l12, l13, l14,l15,\n",
    "                                           l16, l17, l18, l19, l20, l21,l22, l23, l24,l25,l26,l27))\n",
    "dtn_anom_type_data_labels = pd.DataFrame(data=dtn_anom_type_data_labels)\n",
    "dtn_anom_type_data_labels = dtn_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "print(pandas_dtn.shape)\n",
    "print(dtn_anom_type_data_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Datasets (randomized on seed value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dtn_train_data, dtn_test_data, dtn_train_labels, dtn_test_labels = train_test_split(dtn_data, \n",
    "                                                                   dtn_anom_type_data_labels, random_state=seed)\n",
    "dtn_train_data, dtn_train_labels = SMOTE().fit_resample(dtn_train_data, dtn_train_labels)\n",
    "\n",
    "emulab_train_data, emulab_test_data, emulab_train_labels, emulab_test_labels = train_test_split(emulab_data, emulab_anom_type_data_labels\n",
    "                                                                    , random_state=seed)\n",
    "emulab_train_data, emulab_train_labels = SMOTE().fit_resample(emulab_train_data, emulab_train_labels)\n",
    "\n",
    "hpc_train_data, hpc_test_data, hpc_train_labels, hpc_test_labels = train_test_split(hpc_data, \n",
    "                                                                    hpc_anom_type_data_labels, random_state=seed)\n",
    "hpc_train_data, hpc_train_labels = SMOTE().fit_resample(hpc_train_data, hpc_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = dtn_train_data.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(700, activation='relu', input_dim=n_cols))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "dtn_train_labels = to_categorical(dtn_train_labels)\n",
    "hpc_train_labels = to_categorical(hpc_train_labels)\n",
    "emulab_train_labels = to_categorical(emulab_train_labels)\n",
    "\n",
    "model.fit(dtn_train_data, dtn_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "plot_confusion_matrix(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_dtn_data_NN.pdf')\n",
    "plt.show()\n",
    "report = classification_report(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1), output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_dtn_data_NN_CR.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1), output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_hpc_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = model.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1), output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_emulab_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_emulab_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "model.fit(hpc_train_data, hpc_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_dtn_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_dtn_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels,  np.argmax(hpc_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = model.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1), output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_emulab_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_emullab_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "model.fit(emulab_train_data, emulab_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels,  np.argmax(dtn_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_dtn_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_dtn_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels,  np.argmax(hpc_predicted_labels, axis=1) , output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_hpc_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = model.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_emulab_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_emulab_data_NN.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Networks : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn =  DecisionTreeClassifier()\n",
    "dtn.fit(dtn_train_data, dtn_train_labels)\n",
    "dtn_predicted_labels = dtn.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_dtn_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_dtn_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = dtn.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_hpc_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_hpc_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = dtn.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_emulab_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_emulab_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hpc =  DecisionTreeClassifier()\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n",
    "dtn_predicted_labels = hpc.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_dtn_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_dtn_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = hpc.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = hpc.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_emulab_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_emulab_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "emulab = DecisionTreeClassifier()\n",
    "emulab.fit(emulab_train_data, emulab_train_labels)\n",
    "dtn_predicted_labels = emulab.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_dtn_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_dtn_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = emulab.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_hpc_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_hpc_data_DT.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = emulab.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_emulab_data_DT_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_emulab_data_DT.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn =  RandomForestClassifier(n_estimators = 1000, random_state = 42,n_jobs = -1)\n",
    "dtn.fit(dtn_train_data, dtn_train_labels)\n",
    "dtn_predicted_labels = dtn.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_dtn_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_dtn_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = dtn.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_hpc_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_hpc_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "emulab_predicted_labels = dtn.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_emulab_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_emulab_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "hpc =  RandomForestClassifier(n_estimators = 1000, random_state = 42,n_jobs = -1)\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n",
    "dtn_predicted_labels = hpc.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_dtn_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_dtn_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = hpc.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = hpc.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_emulab_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_emulab_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab = RandomForestClassifier(n_estimators = 1000, random_state = 42,n_jobs = -1)\n",
    "emulab.fit(emulab_train_data, emulab_train_labels)\n",
    "dtn_predicted_labels = emulab.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_dtn_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_dtn_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = emulab.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_hpc_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_hpc_data_RF.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = emulab.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_emulab_data_RF_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_emulab_data_RF.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn = svm.SVC(kernel='linear', gamma ='auto', max_iter=10000)\n",
    "dtn.fit(dtn_train_data, dtn_train_labels)\n",
    "dtn_predicted_labels = dtn.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_dtn_data_SVM_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, dtn_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_dtn_data_SVM.pdf')\n",
    "plt.show()\n",
    "hpc_predicted_labels = dtn.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_hpc_data_SVM_CR.csv\", sep='\\t')\n",
    "emulab_predicted_labels = dtn.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_emulab_data_SVM_CR.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "hpc = svm.SVC(kernel='linear', gamma ='auto', max_iter=10000)\n",
    "hpc.fit(hpc_train_data, hpc_train_labels)\n",
    "dtn_predicted_labels = hpc.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_dtn_data_SVM_CR.csv\", sep='\\t')\n",
    "hpc_predicted_labels = hpc.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_SVM_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, hpc_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_SVM.pdf')\n",
    "plt.show()\n",
    "emulab_predicted_labels = hpc.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_emulab_data_SVM_CR.csv\", sep='\\t')\n",
    "\n",
    "emulab = svm.SVC(kernel='linear', gamma ='auto', max_iter=10000)\n",
    "emulab.fit(emulab_train_data, emulab_train_labels)\n",
    "dtn_predicted_labels = emulab.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, dtn_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_dtn_data_SVM_CR.csv\", sep='\\t')\n",
    "hpc_predicted_labels = emulab.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, hpc_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_hpc_data_SVM_CR.csv\", sep='\\t')\n",
    "emulab_predicted_labels = emulab.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, emulab_predicted_labels, output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_emulab_data_SVM_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, emulab_predicted_labels, normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_emulab_data_SVM.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
