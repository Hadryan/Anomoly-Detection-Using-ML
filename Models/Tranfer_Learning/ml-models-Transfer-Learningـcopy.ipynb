{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Indexes\n",
    "This chunk loads a file that contains the labels we want to load from the datasets as well as their indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import re\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import seaborn as sn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "from graphviz import Source\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_wine\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTE, ADASYN)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import backend as K \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "class_names = [\"Normal\", \"Corrupt\", \"Delay\", \"Loss\", \"Duplicate\"]\n",
    "\n",
    "seed = 0\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues, fig_size=(12,10)):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"../../tstat_labels_indexes.txt\" ,'r')\n",
    "data_field_list = []\n",
    "for line in infile.readlines():\n",
    "    if \":\" in line:\n",
    "        data_field = str(re.search('%s(.*)%s' % (\"\\\"\", \"\\\"\"), line).group(1))\n",
    "        index = int(re.search('%s(.*)%s' % (\":\", \",\"), line).group(1))\n",
    "        data_field_list.append((data_field, index))\n",
    "\n",
    "index_to_key_dict = {}\n",
    "key_to_index_dict = {}\n",
    "data_field_labels = []\n",
    "for data_field, index in data_field_list:\n",
    "    key_to_index_dict[data_field] = index\n",
    "    index_to_key_dict[index] = data_field\n",
    "    data_field_labels.append(data_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_file(file_name):\n",
    "    infile = open(file_name, 'r')\n",
    "    header = infile.readline().split(' ')\n",
    "    entries = []\n",
    "    labels = None\n",
    "    for i, line in enumerate(infile.readlines()):\n",
    "        row = get_data_row(line)\n",
    "        row = clean_data_row(row)\n",
    "        if row != []:\n",
    "            entries.append(row)\n",
    "    entries = np.array(entries)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data row\n",
    "Called by the read in file function. Loads a single line from the dataset files. Super inefficient, but only loads labels which are in the data field list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_row(line):\n",
    "    global index_to_key_dict\n",
    "    line = line.split(' ')\n",
    "    row = []\n",
    "    labels = []\n",
    "    c_pkt_cnt = 0\n",
    "    s_pkt_cnt = 0\n",
    "    c_bytes_cnt = 0\n",
    "    s_bytes_cnt = 0\n",
    "    for data_field, index in data_field_list:\n",
    "        #print(\"df:\", data_field,\"ix:\",index)\n",
    "        #print(line)\n",
    "        \n",
    "        \n",
    "        if data_field == \"client_pkt_cnt\":\n",
    "            try:\n",
    "                c_pkt_cnt = line[index]\n",
    "                c_pkt_cnt = max(float(c_pkt_cnt), 1)\n",
    "            except:\n",
    "                c_pkt_cnt = 1\n",
    "            #if c_pkt_cnt < 32:\n",
    "            #    return []\n",
    "        elif data_field == \"serv_pkt_cnt\":\n",
    "            try:\n",
    "                s_pkt_cnt = line[index]\n",
    "                s_pkt_cnt = max(float(s_pkt_cnt), 1)\n",
    "            except:\n",
    "                s_pkt_cnt = 1\n",
    "        elif data_field == \"client_bytes_cnt\":\n",
    "            try:\n",
    "                c_bytes_cnt = line[index]\n",
    "                c_bytes_cnt = max(float(c_bytes_cnt), 1)\n",
    "            except:\n",
    "                c_bytes_cnt = 1\n",
    "        elif data_field == \"serv_bytes_cnt\":\n",
    "            try:\n",
    "                s_bytes_cnt = line[index]\n",
    "                s_bytes_cnt = max(float(s_bytes_cnt), 1)\n",
    "            except:\n",
    "                s_bytes_cnt = 1\n",
    "                \n",
    "    for data_field, index in data_field_list:\n",
    "        try:\n",
    "            val = line[index]\n",
    "            val = float(val)\n",
    "        except:\n",
    "            val = 0\n",
    "        row.append(val)    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data row\n",
    "Not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_row(in_row):\n",
    "    global index_to_key_dict, key_to_index_dict\n",
    "    return in_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset\n",
    "Loads all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packet_count_threshold = 200\n",
    "def get_dataset(path):\n",
    "    out_data = []\n",
    "    file = open('summary.out', 'w')\n",
    "    for sub_dir in os.listdir(path):\n",
    "        temp_path = os.path.join(path, sub_dir)\n",
    "        temp_path = os.path.join(temp_path, \"log_tcp_complete\")\n",
    "        if os.path.isfile(temp_path): \n",
    "            temp_data = np.nan_to_num(read_in_file(temp_path))\n",
    "            rows_before = temp_data.shape[0]\n",
    "            if rows_before == 0:\n",
    "                continue\n",
    "            print()\n",
    "            temp_data = temp_data[temp_data[:,2] > packet_count_threshold]\n",
    "            rows_after = temp_data.shape[0]\n",
    "            print (path + \" removed \" + str(rows_before-rows_after) + \"/\" + str(rows_before) + \" rows\")\n",
    "            if len(temp_data) == 0:\n",
    "                continue\n",
    "            if out_data == []:\n",
    "                out_data = temp_data\n",
    "            else:\n",
    "                out_data = np.concatenate((out_data, temp_data))\n",
    "    file.close\n",
    "    return out_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all datasets\n",
    "Load all datasets\n",
    "Create numerical lables for each class, and a different set of labels for each subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = get_dataset(\"../../DataSet/emulab/normal\")\n",
    "corr_01 = get_dataset(\"../../DataSet/emulab/corrupt_0.1perc\")\n",
    "corr_05 =  get_dataset(\"../../DataSet/emulab/corrupt_5perc\")\n",
    "corr_10 = get_dataset(\"../../DataSet/emulab/corrupt_1.0perc\")\n",
    "delay_1_1 = get_dataset(\"../../DataSet/emulab/delay_1_var_1\")\n",
    "delay_5_2 = get_dataset(\"../../DataSet/emulab/delay_5_var_2\")\n",
    "delay_10_5 = get_dataset(\"../../DataSet/emulab/delay_10_var_5\")\n",
    "delay_25_20 = get_dataset(\"../../DataSet/emulab/delay_25_var_20\")\n",
    "drop_1 = get_dataset(\"../../DataSet/emulab/loss_1_perc\")\n",
    "drop_3 = get_dataset(\"../../DataSet/emulab/loss_5_perc\")\n",
    "drop_6 = get_dataset(\"../../DataSet/emulab/loss_10_perc\")\n",
    "dup_1 = get_dataset(\"../../DataSet/emulab/dup_1perc\")\n",
    "dup_5 = get_dataset(\"../../DataSet/emulab/dup_5perc\")\n",
    "dup_7 = get_dataset(\"../../DataSet/emulab/dup_7perc\")\n",
    "\n",
    "\n",
    "\n",
    "l1  = np.ones(len(normal  ))    *1\n",
    "l2  = np.ones(len(corr_01  ))   *2\n",
    "l3  = np.ones(len(corr_05  ))   *2\n",
    "l4  = np.ones(len(corr_10  ))   *2\n",
    "l5  = np.ones(len(delay_1_1))   *3\n",
    "l6  = np.ones(len(delay_5_2))   *3\n",
    "l7  = np.ones(len(delay_10_5))  *3\n",
    "l8  = np.ones(len(delay_25_20)) *3\n",
    "l9  = np.ones(len(drop_1 ) )    *4\n",
    "l10  = np.ones(len(drop_3 ) )   *4\n",
    "l11  = np.ones(len(drop_6 ) )   *4\n",
    "l12  = np.ones(len(dup_1))      *5\n",
    "l13  = np.ones(len(dup_5))      *5\n",
    "l14  = np.ones(len(dup_7))      *5\n",
    "\n",
    "emulab_normal = pd.DataFrame(data=normal, \n",
    "              columns=data_field_labels)\n",
    "\n",
    "\n",
    "\n",
    "emulab_data = np.concatenate((emulab_normal, corr_01, corr_05, corr_10, delay_1_1, delay_5_2, delay_10_5, delay_25_20,\n",
    "                           drop_1, drop_3, drop_6, dup_1, dup_5, dup_7))\n",
    "print(\"###########\")\n",
    "print(emulab_data.shape)\n",
    "print(\"###########\")\n",
    "emulab_data = pd.DataFrame(data=emulab_data, \n",
    "              columns=data_field_labels)\n",
    "emulab_data = PowerTransformer().fit_transform(emulab_data)\n",
    "# emulab_data = MinMaxScaler().fit_transform(pandas_emulab)\n",
    "\n",
    "\n",
    "emulab_anom_type_data_labels = np.concatenate((l1, l2, l3, l4, l5, l6, l7, l8, l9, l10, l11, l12, l13, l14))\n",
    "emulab_anom_type_data_labels = pd.DataFrame(data=emulab_anom_type_data_labels)\n",
    "emulab_anom_type_data_labels = emulab_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hpc_normal = get_dataset(\"../../DataSet/hpc/normal\")\n",
    "hpc_corr_01 = get_dataset(\"../../DataSet/hpc/corrupt_0.1perc\")\n",
    "hpc_corr_05 = get_dataset(\"../../DataSet/hpc/corrupt_0.5perc\")\n",
    "hpc_corr_10 = get_dataset(\"../../DataSet/hpc/corrupt_1.0perc\")\n",
    "hpc_delay_1_1 = get_dataset(\"../../DataSet/hpc/delay_1_var_1\")\n",
    "hpc_delay_5_2 = get_dataset(\"../../DataSet/hpc/delay_5_var_2\")\n",
    "hpc_delay_10_5 = get_dataset(\"../../DataSet/hpc/delay_10_var_5\")\n",
    "hpc_delay_25_20 = get_dataset(\"../../DataSet/hpc/delay_25_var_20\")\n",
    "hpc_drop_01 = get_dataset(\"../../DataSet/hpc/loss_5perc\")\n",
    "hpc_drop_001 = get_dataset(\"../../DataSet/hpc/loss_10perc\")\n",
    "hpc_drop_0005 = get_dataset(\"../../DataSet/hpc/loss_15perc\")\n",
    "hpc_dup_1 = get_dataset(\"../../DataSet/hpc/dup_10perc\")\n",
    "hpc_dup_2 = get_dataset(\"../../DataSet/hpc/dup_20perc\")\n",
    "\n",
    "\n",
    "hpc_corrupt_001 = get_dataset(\"../../DataSet/hpc/corrupt_0.01\")\n",
    "hpc_corrupt_005 = get_dataset(\"../../DataSet/hpc/corrupt_0.05\")\n",
    "hpc_corrupt_01 = get_dataset(\"../../DataSet/hpc/corrupt_0.1\")\n",
    "hpc_corrupt_05 = get_dataset(\"../../DataSet/hpc/corrupt_0.5\")\n",
    "hpc_duplicate_001 = get_dataset(\"../../DataSet/hpc/duplicate_0.01\")\n",
    "hpc_duplicate_005 = get_dataset(\"../../DataSet/hpc/duplicate_0.05\")\n",
    "hpc_duplicate_01 = get_dataset(\"../../DataSet/hpc/duplicate_0.1\")\n",
    "hpc_duplicate_05 = get_dataset(\"../../DataSet/hpc/duplicate_0.5\")\n",
    "hpc_loss_001 = get_dataset(\"../../DataSet/hpc/loss_0.01\")\n",
    "hpc_loss_005 = get_dataset(\"../../DataSet/hpc/loss_0.05\")\n",
    "hpc_loss_01 = get_dataset(\"../../DataSet/hpc/loss_0.1\")\n",
    "hpc_loss_05 = get_dataset(\"../../DataSet/hpc/loss_0.5\")\n",
    "\n",
    "\n",
    "\n",
    "hpc_l1  = np.ones(len(hpc_normal  ))    *1\n",
    "hpc_l2  = np.ones(len(hpc_corr_01  ))   *2\n",
    "hpc_l3  = np.ones(len(hpc_corr_05  ))   *2\n",
    "hpc_l4  = np.ones(len(hpc_corr_10  ))   *2\n",
    "hpc_l5  = np.ones(len(hpc_corrupt_001  ))   *2\n",
    "hpc_l6  = np.ones(len(hpc_corrupt_005  ))   *2\n",
    "hpc_l7  = np.ones(len(hpc_corrupt_01  ))   *2\n",
    "hpc_l8  = np.ones(len(hpc_corrupt_05  ))   *2\n",
    "hpc_l9  = np.ones(len(hpc_delay_1_1))   *3\n",
    "hpc_l10  = np.ones(len(hpc_delay_5_2))   *3\n",
    "hpc_l11  = np.ones(len(hpc_delay_10_5))  *3\n",
    "hpc_l12  = np.ones(len(hpc_delay_25_20)) *3\n",
    "hpc_l13  = np.ones(len(hpc_drop_01))     *4\n",
    "hpc_l14  = np.ones(len(hpc_drop_001))    *4\n",
    "hpc_l15  = np.ones(len(hpc_drop_0005))   *4\n",
    "hpc_l16  = np.ones(len(hpc_loss_001))   *4\n",
    "hpc_l17  = np.ones(len(hpc_loss_005))   *4\n",
    "hpc_l18  = np.ones(len(hpc_loss_01))   *4\n",
    "hpc_l19  = np.ones(len(hpc_loss_05))   *4\n",
    "hpc_l20  = np.ones(len(hpc_dup_1))       *5\n",
    "hpc_l21  = np.ones(len(hpc_dup_2))       *5\n",
    "hpc_l22  = np.ones(len(hpc_duplicate_001))       *5\n",
    "hpc_l23  = np.ones(len(hpc_duplicate_005))       *5\n",
    "hpc_l24  = np.ones(len(hpc_duplicate_01))       *5\n",
    "hpc_l25  = np.ones(len(hpc_duplicate_05))       *5\n",
    "\n",
    "hpc_data = np.concatenate((hpc_normal, \n",
    "                           hpc_corr_01, hpc_corr_05, hpc_corr_10,\n",
    "                           hpc_corrupt_001,hpc_corrupt_005,hpc_corrupt_01,hpc_corrupt_05,\n",
    "                           hpc_delay_1_1, hpc_delay_5_2,hpc_delay_10_5,hpc_delay_25_20,\n",
    "                           hpc_drop_01, hpc_drop_001, hpc_drop_0005,\n",
    "                           hpc_loss_001,hpc_loss_005,hpc_loss_01,hpc_loss_05,\n",
    "                           hpc_dup_1, hpc_dup_2,hpc_duplicate_001,hpc_duplicate_005,hpc_duplicate_01,hpc_duplicate_05))\n",
    "\n",
    "hpc_anom_type_data_labels = np.concatenate((hpc_l1, hpc_l2, hpc_l3, hpc_l4, hpc_l5, \n",
    "                                            hpc_l6, hpc_l7, hpc_l8, \n",
    "                                            hpc_l9, hpc_l10, hpc_l11, \n",
    "                                            hpc_l12, hpc_l13,hpc_l14, hpc_l15, hpc_l16, \n",
    "                                            hpc_l17, hpc_l18,hpc_l19,hpc_l20, \n",
    "                                            hpc_l21, hpc_l22,hpc_l23,hpc_l24,hpc_l25))\n",
    "\n",
    "hpc_data = pd.DataFrame(data=hpc_data, \n",
    "              columns=data_field_labels)\n",
    "hpc_data = PowerTransformer().fit_transform(hpc_data)\n",
    "# hpc_data = MinMaxScaler().fit_transform(pandas_hpc)\n",
    "\n",
    "\n",
    "hpc_anom_type_data_labels = pd.DataFrame(data=hpc_anom_type_data_labels)\n",
    "hpc_anom_type_data_labels = hpc_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "# print(pandas_hpc.shape)\n",
    "# print(hpc_anom_type_data_labels.shape)\n",
    "\n",
    "dtn_normal = get_dataset(\"../../DataSet/dtn/FINAL_DATA/normal\")\n",
    "dtn_normal2 = get_dataset(\"../../DataSet/dtn/DTN_LONG_DATA/normal\")\n",
    "dtn_corr_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.1perc\")\n",
    "dtn_corr_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.5perc\")\n",
    "dtn_corr_10 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_1.0perc\")\n",
    "dtn_delay_1_1 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_1_var_1\")\n",
    "dtn_delay_5_2 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_5_var_2\")\n",
    "dtn_delay_10_5 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_10_var_5\")\n",
    "dtn_delay_25_20 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/delay_25_var_20\")\n",
    "dtn_drop_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_1perc\")\n",
    "dtn_drop_5 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_5perc\")\n",
    "dtn_drop_10 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_10perc\")\n",
    "dtn_drop_15 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_15perc\")\n",
    "dtn_dup_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_0.1perc\")\n",
    "dtn_dup_1 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_1perc\")\n",
    "dtn_dup_2 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/dup_2perc\")\n",
    "\n",
    "dtn_corrupt_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.01\")\n",
    "dtn_corrupt_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.05\")\n",
    "dtn_corrupt_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.1\")\n",
    "dtn_corrupt_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/corrupt_0.5\")\n",
    "dtn_duplicate_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.01\")\n",
    "dtn_duplicate_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.05\")\n",
    "dtn_duplicate_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.1\")\n",
    "dtn_duplicate_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/duplicate_0.5\")\n",
    "dtn_loss_001 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.01\")\n",
    "dtn_loss_005 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.05\")\n",
    "dtn_loss_01 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.1\")\n",
    "dtn_loss_05 = get_dataset(\"../../DataSet/dtn/FINAL_DATA/loss_0.5\")\n",
    "\n",
    "\n",
    "l1  = np.ones(len(dtn_normal  ) + len(dtn_normal2)) *1\n",
    "l2  = np.ones(len(dtn_corr_01  ))   *2\n",
    "l3  = np.ones(len(dtn_corr_05  ))   *2\n",
    "l4  = np.ones(len(dtn_corr_10  ))   *2\n",
    "l5  = np.ones(len(dtn_corrupt_001  ))  *2\n",
    "l6  = np.ones(len(dtn_corrupt_005  ))  *2\n",
    "l7  = np.ones(len(dtn_corrupt_01  ))   *2\n",
    "l8  = np.ones(len(dtn_corrupt_05  ))   *2\n",
    "l9  = np.ones(len(dtn_delay_1_1))   *3\n",
    "l10  = np.ones(len(dtn_delay_5_2))   *3\n",
    "l11  = np.ones(len(dtn_delay_10_5))  *3\n",
    "l12  = np.ones(len(dtn_delay_25_20)) *3\n",
    "l13  = np.ones(len(dtn_drop_01 ) ) *4\n",
    "l14  = np.ones(len(dtn_drop_5) )   *4\n",
    "l15  = np.ones(len(dtn_drop_10) )  *4\n",
    "l16 = np.ones(len(dtn_drop_15) )   *4\n",
    "l17  = np.ones(len(dtn_loss_001))  *4\n",
    "l18  = np.ones(len(dtn_loss_005))  *4\n",
    "l19  = np.ones(len(dtn_loss_01))   *4\n",
    "l20  = np.ones(len(dtn_loss_05))   *4\n",
    "l21  = np.ones(len(dtn_dup_01))    *5\n",
    "l22  = np.ones(len(dtn_dup_1))     *5\n",
    "l23  = np.ones(len(dtn_dup_2))     *5\n",
    "l24  = np.ones(len(dtn_duplicate_001)) *5\n",
    "l25  = np.ones(len(dtn_duplicate_005))*5\n",
    "l26  = np.ones(len(dtn_duplicate_01)) *5\n",
    "l27  = np.ones(len(dtn_duplicate_05)) *5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dtn_data = np.concatenate((dtn_normal, dtn_normal2,\n",
    "                           dtn_corr_01, dtn_corr_05, dtn_corr_10,\n",
    "                           dtn_corrupt_001, dtn_corrupt_005, dtn_corrupt_01, dtn_corrupt_05, \n",
    "                           dtn_delay_1_1, dtn_delay_5_2,dtn_delay_10_5,dtn_delay_25_20,\n",
    "                           dtn_drop_01, dtn_drop_5, dtn_drop_10, dtn_drop_15, dtn_loss_001,\n",
    "                           dtn_loss_005, dtn_loss_01, dtn_loss_05,                          \n",
    "                           dtn_dup_01,dtn_dup_1, dtn_dup_2,dtn_duplicate_001,\n",
    "                           dtn_duplicate_005, dtn_duplicate_01, dtn_duplicate_05))\n",
    "\n",
    "\n",
    "# dtn_data = np.concatenate((dtn_normal, dtn_normal2,\n",
    "#                            dtn_corrupt_001, dtn_corrupt_005, dtn_corrupt_01, dtn_corrupt_05, \n",
    "#                            dtn_delay_1_1, dtn_delay_5_2,dtn_delay_10_5,dtn_delay_25_20\n",
    "#                            , dtn_loss_001,\n",
    "#                            dtn_loss_005, dtn_loss_01, dtn_loss_05,                          \n",
    "#                            dtn_duplicate_001,\n",
    "#                            dtn_duplicate_005, dtn_duplicate_01, dtn_duplicate_05))\n",
    "print(\"###########\")\n",
    "print(data_field_labels)\n",
    "print(\"###########\")\n",
    "dtn_data = pd.DataFrame(data=dtn_data, \n",
    "              columns=data_field_labels)\n",
    "dtn_data = PowerTransformer().fit_transform(dtn_data)\n",
    "# dtn_data = MinMaxScaler().fit_transform(pandas_dtn)\n",
    "\n",
    "\n",
    "\n",
    "dtn_anom_type_data_labels = np.concatenate((l1,l2,l3,l4, l5, l6, l7, l8, l9, l10, l11, l12,l13,l14,l15,l16,l17, l18, l19, l20,l21,l22,l23,l24,l25,l26,l27))\n",
    "dtn_anom_type_data_labels = pd.DataFrame(data=dtn_anom_type_data_labels)\n",
    "dtn_anom_type_data_labels = dtn_anom_type_data_labels.values.ravel()\n",
    "\n",
    "\n",
    "# print(pandas_dtn.shape)\n",
    "# print(dtn_anom_type_data_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Datasets (randomized on seed value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_combine_date =  np.concatenate((dtn_data,emulab_data))\n",
    "# new_combine_label =  np.concatenate((dtn_anom_type_data_labels,emulab_anom_type_data_labels))\n",
    "\n",
    "# new_train_data, new_test_data, new_train_labels, new_test_labels = train_test_split(new_combine_date, \n",
    "#                                                                    new_combine_label, random_state=seed)\n",
    "# new_train_data, new_train_labels = SMOTE().fit_resample(new_train_data, new_train_labels)\n",
    "\n",
    "\n",
    "dtn_train_data, dtn_test_data, dtn_train_labels, dtn_test_labels = train_test_split(dtn_data, \n",
    "                                                                   dtn_anom_type_data_labels, random_state=seed)\n",
    "dtn_train_data, dtn_train_labels = SMOTE().fit_resample(dtn_train_data, dtn_train_labels)\n",
    "\n",
    "emulab_train_data, emulab_test_data, emulab_train_labels, emulab_test_labels = train_test_split(emulab_data, emulab_anom_type_data_labels\n",
    "                                                                    , random_state=seed)\n",
    "emulab_train_data, emulab_train_labels = SMOTE().fit_resample(emulab_train_data, emulab_train_labels)\n",
    "\n",
    "hpc_train_data, hpc_test_data, hpc_train_labels, hpc_test_labels = train_test_split(hpc_data, \n",
    "                                                                    hpc_anom_type_data_labels, random_state=seed)\n",
    "hpc_train_data, hpc_train_labels = SMOTE().fit_resample(hpc_train_data, hpc_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = dtn_train_data.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(700, activation='relu', input_dim=n_cols))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "# dtn_train_labels = to_categorical(dtn_train_labels)\n",
    "# hpc_train_labels = to_categorical(hpc_train_labels)\n",
    "# emulab_train_labels = to_categorical(emulab_train_labels)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(dtn_train_data, dtn_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "plot_confusion_matrix(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_dtn_data_NN.pdf')\n",
    "plt.show()\n",
    "report = classification_report(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1), output_dict=True)\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_dtn_data_NN_CR.csv\", sep='\\t')\n",
    "\n",
    "\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1), output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_hpc_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = model.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1), output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"dtn_model_emulab_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('dtn_model_emulab_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = dtn_train_data.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(700, activation='relu', input_dim=n_cols))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "#K.clear_session()\n",
    "model.fit(hpc_train_data, hpc_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_dtn_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_dtn_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels,  np.argmax(hpc_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_hpc_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = model.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1), output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"hpc_model_emulab_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('hpc_model_emullab_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "#create model\n",
    "model = Sequential()\n",
    "\n",
    "#get number of columns in training data\n",
    "n_cols = dtn_train_data.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(Dense(700, activation='relu', input_dim=n_cols))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "\n",
    "#K.clear_session()\n",
    "model.fit(emulab_train_data, emulab_train_labels, validation_split=0.2, epochs=10, callbacks=[early_stopping_monitor])\n",
    "dtn_predicted_labels = model.predict(dtn_test_data)\n",
    "report = classification_report(dtn_test_labels,  np.argmax(dtn_predicted_labels, axis=1), output_dict=True  )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_dtn_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(dtn_test_labels, np.argmax(dtn_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_dtn_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "hpc_predicted_labels = model.predict(hpc_test_data)\n",
    "report = classification_report(hpc_test_labels,  np.argmax(hpc_predicted_labels, axis=1) , output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_hpc_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(hpc_test_labels, np.argmax(hpc_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_hpc_data_NN.pdf')\n",
    "plt.show()\n",
    "\n",
    "emulab_predicted_labels = model.predict(emulab_test_data)\n",
    "report = classification_report(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , output_dict=True )\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(\"emulab_model_emulab_data_NN_CR.csv\", sep='\\t')\n",
    "plot_confusion_matrix(emulab_test_labels, np.argmax(emulab_predicted_labels, axis=1) , normalize=True,classes=class_names, title='')\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "plt.savefig('emulab_model_emulab_data_NN.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "df_train_normed_summary = shap.kmeans(dtn_test_data[:200,:], 10)\n",
    "\n",
    "explainer = shap.KernelExplainer(model.predict,df_train_normed_summary)\n",
    "shap_values_dtn = explainer.shap_values(dtn_test_data[:200])\n",
    "\n",
    "\n",
    "# # we use the first 100 training examples as our background dataset to integrate over\n",
    "# explainer = shap.DeepExplainer(model, dtn_train_data[:500])\n",
    "\n",
    "# # explain the first 10 predictions\n",
    "# # explaining each prediction requires 2 * background dataset size runs\n",
    "# shap_values = explainer.shap_values(dtn_train_data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn_test_data[:200][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_dtn, dtn_test_data,feature_names = data_field_labels )\n",
    "shap.summary_plot(shap_values_dtn[1], dtn_test_data[:200],plot_type = \"dot\",feature_names = data_field_labels)\n",
    "shap.summary_plot(shap_values_dtn[5], dtn_test_data[:200],plot_type = \"dot\",feature_names = data_field_labels)\n",
    "\n",
    "\n",
    "\n",
    "shap.summary_plot(shap_values_dtn[5][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1][:2], dtn_test_data[:200][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1][:2],plot_type = \"dot\",feature_names = data_field_labels)\n",
    "print(model.predict(dtn_test_data[:200][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1][:2]).argmax(axis=1))\n",
    "# print(dtn_test_data[:200][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1][:1][\"c_avg_rrt\"])\n",
    "# dtn_test_data.iloc[:200][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1][:1][\"c_avg_rrt\"] = 1.4\n",
    "# print(dtn_test_data[:200][(dtn_train_labels[:200] == 1).argmax(axis=1) == 1][:1][\"c_avg_rrt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtn_test_data = pd.DataFrame(data=dtn_test_data, \n",
    "              columns=data_field_labels)\n",
    "# print(dtn_test_data.iloc[17,:])\n",
    "print(model.predict(dtn_test_data.iloc[12,:].values.reshape(1,89)).argmax(axis=1))\n",
    "print(dtn_test_labels[12])\n",
    "print(dtn_test_data.iloc[12][\"c_pkts_push\"])\n",
    "\n",
    "print([(dtn_train_labels[:200] == 1).argmax(axis=1) == 1])\n",
    "\n",
    "\n",
    "\n",
    "# print(model.predict().argmax(axis=1))\n",
    "\n",
    "\n",
    "shap.force_plot(explainer.expected_value[5]\n",
    "                , shap_values_dtn[5][12,:]\n",
    "                , dtn_test_data.iloc[12,:], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_normed_summary_emulab = shap.kmeans(emulab_train_data[:200,:], 10)\n",
    "\n",
    "explainer_emulab = shap.KernelExplainer(model.predict,df_train_normed_summary_emulab)\n",
    "shap_values = explainer_emulab.shap_values(emulab_train_data[:200,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, emulab_train_data[:200],feature_names = data_field_labels )\n",
    "shap.summary_plot(shap_values[1], emulab_train_data[:200],plot_type = \"dot\",feature_names = data_field_labels)\n",
    "shap.summary_plot(shap_values[5], emulab_train_data[:200],plot_type = \"dot\",feature_names = data_field_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (emulab_train_labels == 1).argmax(axis=1) == 5\n",
    "emulab_train_data = pd.DataFrame(data=emulab_train_data, \n",
    "              columns=data_field_labels)\n",
    "dtn_train_data = pd.DataFrame(data=dtn_train_data, \n",
    "              columns=data_field_labels)\n",
    "\n",
    "hpc_train_data = pd.DataFrame(data=hpc_train_data, \n",
    "              columns=data_field_labels)\n",
    "\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 1][\"c_pkts_ooo\"].mean())\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 5][\"c_pkts_ooo\"].mean())\n",
    "\n",
    "\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 1][\"c_pkts_ooo\"].mean())\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 5][\"c_pkts_ooo\"].mean())\n",
    "\n",
    "\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 1][\"c_pkts_ooo\"].mean())\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 5][\"c_pkts_ooo\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "print(\"##################################\")\n",
    "\n",
    "\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 1][\"c_avg_rrt\"].mean())\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 5][\"c_avg_rrt\"].mean())\n",
    "\n",
    "\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 1][\"c_avg_rrt\"].mean())\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 5][\"c_avg_rrt\"].mean())\n",
    "\n",
    "\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 1][\"c_avg_rrt\"].mean())\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 5][\"c_avg_rrt\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"##################################\")\n",
    "\n",
    "\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 1][\"c_pkt_reor\"].mean())\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 5][\"c_pkt_reor\"].mean())\n",
    "\n",
    "\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 1][\"c_pkt_reor\"].mean())\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 5][\"c_pkt_reor\"].mean())\n",
    "\n",
    "\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 1][\"c_pkt_reor\"].mean())\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 5][\"c_pkt_reor\"].mean())\n",
    "\n",
    "\n",
    "\n",
    "print(\"##################################\")\n",
    "\n",
    "\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 1][\"c_pkts_push\"].mean())\n",
    "print (emulab_train_data[(emulab_train_labels == 1).argmax(axis=1) == 5][\"c_pkts_push\"].mean())\n",
    "\n",
    "\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 1][\"c_pkts_push\"].mean())\n",
    "print (dtn_train_data[(dtn_train_labels == 1).argmax(axis=1) == 5][\"c_pkts_push\"].mean())\n",
    "\n",
    "\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 1][\"c_pkts_push\"].mean())\n",
    "print (hpc_train_data[(hpc_train_labels == 1).argmax(axis=1) == 5][\"c_pkts_push\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emulab_train_data = pd.DataFrame(data=emulab_train_data, \n",
    "              columns=data_field_labels)\n",
    "print(model.predict(emulab_train_data.iloc[21,:].values.reshape(1,89)).argmax(axis=1))\n",
    "print(emulab_train_labels[21])\n",
    "\n",
    "shap.force_plot(explainer_emulab.expected_value[1]\n",
    "                , shap_values[1][21,:]\n",
    "                , emulab_train_data.iloc[21,:], link=\"logit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
